[{"title": "Draft & Verify: Lossless Large Language Model Acceleration via\n  Self-Speculative Decoding", "abstract": "We present a novel inference scheme, self-speculative decoding, for\naccelerating Large Language Models (LLMs) without the need for an auxiliary\nmodel. This approach is characterized by a two-stage process: drafting and\nverification. The drafting stage generates draft tokens at a slightly lower\nquality but more quickly, which is achieved by selectively skipping certain\nintermediate layers during drafting Subsequently, the verification stage\nemploys the original LLM to validate those draft output tokens in one forward\npass. This process ensures the final output remains identical to that produced\nby the unaltered LLM, thereby maintaining output quality. The proposed method\nrequires no additional neural network training and no extra memory footprint,\nmaking it a plug-and-play and cost-effective solution for inference\nacceleration. Benchmarks with LLaMA-2 and its fine-tuned models demonstrated a\nspeedup up to 1.73$\\times$.", "pdf_link": "http://arxiv.org/pdf/2309.08168v1"}, {"title": "How to Build Low-cost Networks for Large Language Models (without\n  Sacrificing Performance)?", "abstract": "This paper challenges the well-established paradigm for building any-to-any\nnetworks for training Large Language Models (LLMs). We show that LLMs exhibit a\nunique communication pattern where only small groups of GPUs require\nhigh-bandwidth communication to achieve near-optimal training performance.\nAcross these groups of GPUs, the communication is insignificant and\nhomogeneous. We propose a new network architecture that resembles the\ncommunication requirement of LLMs. Our architecture partitions the cluster into\nsets of GPUs interconnected with non-blocking any-to-any high-bandwidth\ninterconnects that we call HB domains. Across the HB domains, the network only\nconnects GPUs with non-zero communication demands. We develop an analytical\nformulation of the training iteration time to evaluate our proposal. Our\nformulation closely estimates the hardware floating-point utilization within\n0.15\\% from the ground truth established in prior studies for larger models. We\nshow that our proposed architecture reduces the network cost by 37% to 75%\ncompared to the state-of-the-art any-to-any Clos networks without compromising\nthe performance of LLM training.", "pdf_link": "http://arxiv.org/pdf/2307.12169v3"}, {"title": "Pathways: Asynchronous Distributed Dataflow for ML", "abstract": "We present the design of a new large scale orchestration layer for\naccelerators. Our system, Pathways, is explicitly designed to enable\nexploration of new systems and ML research ideas, while retaining state of the\nart performance for current models. Pathways uses a sharded dataflow graph of\nasynchronous operators that consume and produce futures, and efficiently\ngang-schedules heterogeneous parallel computations on thousands of accelerators\nwhile coordinating data transfers over their dedicated interconnects. Pathways\nmakes use of a novel asynchronous distributed dataflow design that lets the\ncontrol plane execute in parallel despite dependencies in the data plane. This\ndesign, with careful engineering, allows Pathways to adopt a single-controller\nmodel that makes it easier to express complex new parallelism patterns. We\ndemonstrate that Pathways can achieve performance parity (~100% accelerator\nutilization) with state-of-the-art systems when running SPMD computations over\n2048 TPUs, while also delivering throughput comparable to the SPMD case for\nTransformer models that are pipelined across 16 stages, or sharded across two\nislands of accelerators connected over a data center network.", "pdf_link": "http://arxiv.org/pdf/2203.12533v1"}, {"title": "Scalable and Efficient MoE Training for Multitask Multilingual Models", "abstract": "The Mixture of Experts (MoE) models are an emerging class of sparsely\nactivated deep learning models that have sublinear compute costs with respect\nto their parameters. In contrast with dense models, the sparse architecture of\nMoE offers opportunities for drastically growing model size with significant\naccuracy gain while consuming much lower compute budget. However, supporting\nlarge scale MoE training also has its own set of system and modeling\nchallenges. To overcome the challenges and embrace the opportunities of MoE, we\nfirst develop a system capable of scaling MoE models efficiently to trillions\nof parameters. It combines multi-dimensional parallelism and heterogeneous\nmemory technologies harmoniously with MoE to empower 8x larger models on the\nsame hardware compared with existing work. Besides boosting system efficiency,\nwe also present new training methods to improve MoE sample efficiency and\nleverage expert pruning strategy to improve inference time efficiency. By\ncombining the efficient system and training methods, we are able to\nsignificantly scale up large multitask multilingual models for language\ngeneration which results in a great improvement in model accuracy. A model\ntrained with 10 billion parameters on 50 languages can achieve state-of-the-art\nperformance in Machine Translation (MT) and multilingual natural language\ngeneration tasks. The system support of efficient MoE training has been\nimplemented and open-sourced with the DeepSpeed library.", "pdf_link": "http://arxiv.org/pdf/2109.10465v1"}, {"title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding", "abstract": "This work aims at decreasing the end-to-end generation latency of large\nlanguage models (LLMs). One of the major causes of the high generation latency\nis the sequential decoding approach adopted by almost all state-of-the-art\nLLMs. In this work, motivated by the thinking and writing process of humans, we\npropose Skeleton-of-Thought (SoT), which first guides LLMs to generate the\nskeleton of the answer, and then conducts parallel API calls or batched\ndecoding to complete the contents of each skeleton point in parallel. Not only\ndoes SoT provide considerable speed-ups across 12 LLMs, but it can also\npotentially improve the answer quality on several question categories. SoT is\nan initial attempt at data-centric optimization for inference efficiency, and\nfurther underscores the potential of pushing LLMs to think more like a human\nfor answer quality.", "pdf_link": "http://arxiv.org/pdf/2307.15337v2"}, {"title": "Cramming: Training a Language Model on a Single GPU in One Day", "abstract": "Recent trends in language modeling have focused on increasing performance\nthrough scaling, and have resulted in an environment where training language\nmodels is out of reach for most researchers and practitioners. While most in\nthe community are asking how to push the limits of extreme computation, we ask\nthe opposite question: How far can we get with a single GPU in just one day?\n  We investigate the downstream performance achievable with a transformer-based\nlanguage model trained completely from scratch with masked language modeling\nfor a single day on a single consumer GPU. Aside from re-analyzing nearly all\ncomponents of the pretraining pipeline for this scenario and providing a\nmodified pipeline with performance close to BERT, we investigate why scaling\ndown is hard, and which modifications actually improve performance in this\nscenario. We provide evidence that even in this constrained setting,\nperformance closely follows scaling laws observed in large-compute settings.\nThrough the lens of scaling laws, we categorize a range of recent improvements\nto training and architecture and discuss their merit and practical\napplicability (or lack thereof) for the limited compute setting.", "pdf_link": "http://arxiv.org/pdf/2212.14034v1"}, {"title": "Fast Distributed Inference Serving for Large Language Models", "abstract": "Large language models (LLMs) power a new generation of interactive AI\napplications exemplified by ChatGPT. The interactive nature of these\napplications demand low job completion time (JCT) for model inference. Existing\nLLM serving systems use run-to-completion processing for inference jobs, which\nsuffers from head-of-line blocking and long JCT. We present FastServe, a\ndistributed inference serving system for LLMs. FastServe exploits the\nautoregressive pattern of LLM inference to enable preemption at the granularity\nof each output token. FastServe uses preemptive scheduling to minimize JCT with\na novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi\ninformation-agnostic setting of LLM inference, the scheduler leverages the\ninput length information to assign an appropriate initial queue for each\narrival job to join. The higher priority queues than the joined queue are\nskipped to reduce demotions. We design an efficient GPU memory management\nmechanism that proactively offloads and uploads intermediate states between GPU\nmemory and host memory for LLM inference. We build a system prototype of\nFastServe based on NVIDIA FasterTransformer. Experimental results show that\ncompared to the state-of-the-art solution Orca, FastServe improves the average\nand tail JCT by up to 5.1$\\times$ and 6.4$\\times$, respectively.", "pdf_link": "http://arxiv.org/pdf/2305.05920v1"}, {"title": "Scaling Laws for Neural Language Models", "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.", "pdf_link": "http://arxiv.org/pdf/2001.08361v1"}, {"title": "FLM-101B: An Open LLM and How to Train It with $100K Budget", "abstract": "Large language models (LLMs) have achieved remarkable success in NLP and\nmultimodal tasks, among others. Despite these successes, two main challenges\nremain in developing LLMs: (i) high computational cost, and (ii) fair and\nobjective evaluations. In this paper, we report a solution to significantly\nreduce LLM training cost through a growth strategy. We demonstrate that a\n101B-parameter LLM with 0.31T tokens can be trained with a budget of 100K US\ndollars. Inspired by IQ tests, we also consolidate an additional range of\nevaluations on top of existing evaluations that focus on knowledge-oriented\nabilities. These IQ evaluations include symbolic mapping, rule understanding,\npattern mining, and anti-interference. Such evaluations minimize the potential\nimpact of memorization. Experimental results show that our model, named\nFLM-101B, trained with a budget of 100K US dollars, achieves performance\ncomparable to powerful and well-known models, e.g., GPT-3 and GLM-130B,\nespecially on the additional range of IQ evaluations. The checkpoint of\nFLM-101B is released at https://huggingface.co/CofeAI/FLM-101B.", "pdf_link": "http://arxiv.org/pdf/2309.03852v2"}, {"title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks\nbased on a few demonstrations or natural language instructions. While these\ncapabilities have led to widespread adoption, most LLMs are developed by\nresource-rich organizations and are frequently kept from the public. As a step\ntowards democratizing this powerful technology, we present BLOOM, a\n176B-parameter open-access language model designed and built thanks to a\ncollaboration of hundreds of researchers. BLOOM is a decoder-only Transformer\nlanguage model that was trained on the ROOTS corpus, a dataset comprising\nhundreds of sources in 46 natural and 13 programming languages (59 in total).\nWe find that BLOOM achieves competitive performance on a wide variety of\nbenchmarks, with stronger results after undergoing multitask prompted\nfinetuning. To facilitate future research and applications using LLMs, we\npublicly release our models and code under the Responsible AI License.", "pdf_link": "http://arxiv.org/pdf/2211.05100v4"}, {"title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\n  Model Parallelism", "abstract": "Recent work in language modeling demonstrates that training large transformer\nmodels advances the state of the art in Natural Language Processing\napplications. However, very large models can be quite difficult to train due to\nmemory constraints. In this work, we present our techniques for training very\nlarge transformer models and implement a simple, efficient intra-layer model\nparallel approach that enables training transformer models with billions of\nparameters. Our approach does not require a new compiler or library changes, is\northogonal and complimentary to pipeline model parallelism, and can be fully\nimplemented with the insertion of a few communication operations in native\nPyTorch. We illustrate this approach by converging transformer based models up\nto 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the\nentire application with 76% scaling efficiency when compared to a strong single\nGPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can further advance the state of the art\n(SOTA), we train an 8.3 billion parameter transformer language model similar to\nGPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful\nattention to the placement of layer normalization in BERT-like models is\ncritical to achieving increased performance as the model size grows. Using the\nGPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA\nperplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)\ndatasets. Our BERT model achieves SOTA results on the RACE dataset (90.9%\ncompared to SOTA accuracy of 89.4%).", "pdf_link": "http://arxiv.org/pdf/1909.08053v4"}, {"title": "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A\n  Large-Scale Generative Language Model", "abstract": "Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.", "pdf_link": "http://arxiv.org/pdf/2201.11990v3"}, {"title": "FastMoE: A Fast Mixture-of-Expert Training System", "abstract": "Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of\nlanguage model to trillions of parameters. However, training trillion-scale MoE\nrequires algorithm and system co-design for a well-tuned high performance\ndistributed training system. Unfortunately, the only existing platform that\nmeets the requirements strongly depends on Google's hardware (TPU) and software\n(Mesh Tensorflow) stack, and is not open and available to the public,\nespecially GPU and PyTorch communities.\n  In this paper, we present FastMoE, a distributed MoE training system based on\nPyTorch with common accelerators. The system provides a hierarchical interface\nfor both flexible model design and easy adaption to different applications,\nsuch as Transformer-XL and Megatron-LM. Different from direct implementation of\nMoE models using PyTorch, the training speed is highly optimized in FastMoE by\nsophisticated high-performance acceleration skills. The system supports placing\ndifferent experts on multiple GPUs across multiple nodes, enabling enlarging\nthe number of experts linearly against the number of GPUs. The source of\nFastMoE is available at https://github.com/laekov/fastmoe under Apache-2\nlicense.", "pdf_link": "http://arxiv.org/pdf/2103.13262v1"}, {"title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens", "abstract": "Scaling sequence length has become a critical demand in the era of large\nlanguage models. However, existing methods struggle with either computational\ncomplexity or model expressivity, rendering the maximum sequence length\nrestricted. To address this issue, we introduce LongNet, a Transformer variant\nthat can scale sequence length to more than 1 billion tokens, without\nsacrificing the performance on shorter sequences. Specifically, we propose\ndilated attention, which expands the attentive field exponentially as the\ndistance grows. LongNet has significant advantages: 1) it has a linear\ncomputation complexity and a logarithm dependency between any two tokens in a\nsequence; 2) it can be served as a distributed trainer for extremely long\nsequences; 3) its dilated attention is a drop-in replacement for standard\nattention, which can be seamlessly integrated with the existing\nTransformer-based optimization. Experiments results demonstrate that LongNet\nyields strong performance on both long-sequence modeling and general language\ntasks. Our work opens up new possibilities for modeling very long sequences,\ne.g., treating a whole corpus or even the entire Internet as a sequence.", "pdf_link": "http://arxiv.org/pdf/2307.02486v2"}, {"title": "Effective Long-Context Scaling of Foundation Models", "abstract": "We present a series of long-context LLMs that support effective context\nwindows of up to 32,768 tokens. Our model series are built through continual\npretraining from Llama 2 with longer training sequences and on a dataset where\nlong texts are upsampled. We perform extensive evaluation on language modeling,\nsynthetic context probing tasks, and a wide range of research benchmarks. On\nresearch benchmarks, our models achieve consistent improvements on most regular\ntasks and significant improvements on long-context tasks over Llama 2. Notably,\nwith a cost-effective instruction tuning procedure that does not require\nhuman-annotated long instruction data, the 70B variant can already surpass\ngpt-3.5-turbo-16k's overall performance on a suite of long-context tasks.\nAlongside these results, we provide an in-depth analysis on the individual\ncomponents of our method. We delve into Llama's position encodings and discuss\nits limitation in modeling long dependencies. We also examine the impact of\nvarious design choices in the pretraining process, including the data mix and\nthe training curriculum of sequence lengths -- our ablation experiments suggest\nthat having abundant long texts in the pretrain dataset is not the key to\nachieving strong performance, and we empirically verify that long context\ncontinual pretraining is more efficient and similarly effective compared to\npretraining from scratch with long sequences.", "pdf_link": "http://arxiv.org/pdf/2309.16039v3"}, {"title": "Scaling TransNormer to 175 Billion Parameters", "abstract": "We present TransNormerLLM, the first linear attention-based Large Language\nModel (LLM) that outperforms conventional softmax attention-based models in\nterms of both accuracy and efficiency. TransNormerLLM evolves from the previous\nlinear attention architecture TransNormer by making advanced modifications that\ninclude positional embedding, linear attention acceleration, gating mechanism,\ntensor normalization, inference acceleration and stabilization. Specifically,\nwe use LRPE together with an exponential decay to avoid attention dilution\nissues while allowing the model to retain global interactions between tokens.\nAdditionally, we propose Lightning Attention, a cutting-edge technique that\naccelerates linear attention by more than twice in runtime and reduces memory\nusage by a remarkable four times. To further enhance the performance of\nTransNormer, we leverage a gating mechanism to smooth training and a new tensor\nnormalization scheme to accelerate the model, resulting in an impressive\nacceleration of over 20%. Furthermore, we have developed a robust inference\nalgorithm that ensures numerical stability and consistent inference speed,\nregardless of the sequence length, showcasing superior efficiency during both\ntraining and inference stages. Scalability is at the heart of our model's\ndesign, enabling seamless deployment on large-scale clusters and facilitating\nexpansion to even more extensive models, all while maintaining outstanding\nperformance metrics. Rigorous validation of our model design is achieved\nthrough a series of comprehensive experiments on our self-collected corpus,\nboasting a size exceeding 6TB and containing over 2 trillion tokens. To ensure\ndata quality and relevance, we implement a new self-cleaning strategy to filter\nour collected data. Our pre-trained models will be released to foster community\nadvancements in efficient LLMs.", "pdf_link": "http://arxiv.org/pdf/2307.14995v1"}, {"title": "ByteTransformer: A High-Performance Transformer Boosted for\n  Variable-Length Inputs", "abstract": "Transformers have become keystone models in natural language processing over\nthe past decade. They have achieved great popularity in deep learning\napplications, but the increasing sizes of the parameter spaces required by\ntransformer models generate a commensurate need to accelerate performance.\nNatural language processing problems are also routinely faced with\nvariable-length sequences, as word counts commonly vary among sentences.\nExisting deep learning frameworks pad variable-length sequences to a maximal\nlength, which adds significant memory and computational overhead. In this\npaper, we present ByteTransformer, a high-performance transformer boosted for\nvariable-length inputs. We propose a padding-free algorithm that liberates the\nentire transformer from redundant computations on zero padded tokens. In\naddition to algorithmic-level optimization, we provide architecture-aware\noptimizations for transformer functional modules, especially the\nperformance-critical algorithm Multi-Head Attention (MHA). Experimental results\non an NVIDIA A100 GPU with variable-length sequence inputs validate that our\nfused MHA outperforms PyTorch by 6.13x. The end-to-end performance of\nByteTransformer for a forward BERT transformer surpasses state-of-the-art\ntransformer frameworks, such as PyTorch JIT, TensorFlow XLA, Tencent\nTurboTransformer, Microsoft DeepSpeed-Inference and NVIDIA FasterTransformer,\nby 87\\%, 131\\%, 138\\%, 74\\% and 55\\%, respectively. We also demonstrate the\ngeneral applicability of our optimization methods to other BERT-like models,\nincluding ALBERT, DistilBERT, and DeBERTa.", "pdf_link": "http://arxiv.org/pdf/2210.03052v4"}, {"title": "Training Compute-Optimal Large Language Models", "abstract": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.", "pdf_link": "http://arxiv.org/pdf/2203.15556v1"}, {"title": "HyperAttention: Long-context Attention in Near-Linear Time", "abstract": "We present an approximate attention mechanism named HyperAttention to address\nthe computational challenges posed by the growing complexity of long contexts\nused in Large Language Models (LLMs). Recent work suggests that in the\nworst-case scenario, quadratic time is necessary unless the entries of the\nattention matrix are bounded or the matrix has low stable rank. We introduce\ntwo parameters which measure: (1) the max column norm in the normalized\nattention matrix, and (2) the ratio of row norms in the unnormalized attention\nmatrix after detecting and removing large entries. We use these fine-grained\nparameters to capture the hardness of the problem. Despite previous lower\nbounds, we are able to achieve a linear time sampling algorithm even when the\nmatrix has unbounded entries or a large stable rank, provided the above\nparameters are small. HyperAttention features a modular design that easily\naccommodates integration of other fast low-level implementations, particularly\nFlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to\nidentify large entries, HyperAttention outperforms existing methods, giving\nsignificant speed improvements compared to state-of-the-art solutions like\nFlashAttention. We validate the empirical performance of HyperAttention on a\nvariety of different long-context length datasets. For example, HyperAttention\nmakes the inference time of ChatGLM2 50\\% faster on 32k context length while\nperplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,\nwith causal masking, HyperAttention offers 5-fold speedup on a single attention\nlayer.", "pdf_link": "http://arxiv.org/pdf/2310.05869v2"}, {"title": "EnergonAI: An Inference System for 10-100 Billion Parameter Transformer\n  Models", "abstract": "Large transformer models display promising performance on a wide range of\nnatural language processing (NLP) tasks. Although the AI community has expanded\nthe model scale to the trillion parameter level, the practical deployment of\n10-100 billion parameter models is still uncertain due to the latency,\nthroughput, and memory constraints.\n  In this paper, we proposed EnergonAI to solve the challenges of the efficient\ndeployment of 10-100 billion parameter transformer models on single- or\nmulti-GPU systems. EnergonAI adopts a hierarchy-controller system architecture\nto coordinate multiple devices and efficiently support different parallel\npatterns. It delegates the execution of sub-models to multiple workers in the\nsingle-controller style and applies tensor parallelism and pipeline parallelism\namong the workers in a multi-controller style. Upon the novel architecture, we\npropose three techniques, i.e. non-blocking pipeline parallelism, distributed\nredundant computation elimination, and peer memory pooling. EnergonAI enables\nthe users to program complex parallel code the same as a serial one. Compared\nwith the FasterTransformer, we have proven that EnergonAI has superior\nperformance on latency and throughput. In our experiments, EnergonAI can\nachieve 37% latency reduction in tensor parallelism, 10% scalability\nimprovement in pipeline parallelism, and it improves the model scale inferred\non a single GPU by using a larger heterogeneous memory space at cost of limited\nperformance reduction.", "pdf_link": "http://arxiv.org/pdf/2209.02341v1"}, {"title": "Accelerating LLM Inference with Staged Speculative Decoding", "abstract": "Recent advances with large language models (LLM) illustrate their diverse\ncapabilities. We propose a novel algorithm, staged speculative decoding, to\naccelerate LLM inference in small-batch, on-device scenarios. We address the\nlow arithmetic intensity of small-batch inference by improving upon previous\nwork in speculative decoding. First, we restructure the speculative batch as a\ntree, which reduces generation costs and increases the expected tokens per\nbatch. Second, we add a second stage of speculative decoding. Taken together,\nwe reduce single-batch decoding latency by 3.16x with a 762M parameter GPT-2-L\nmodel while perfectly preserving output quality.", "pdf_link": "http://arxiv.org/pdf/2308.04623v1"}, {"title": "Challenges and Applications of Large Language Models", "abstract": "Large Language Models (LLMs) went from non-existent to ubiquitous in the\nmachine learning discourse within a few years. Due to the fast pace of the\nfield, it is difficult to identify the remaining challenges and already\nfruitful application areas. In this paper, we aim to establish a systematic set\nof open problems and application successes so that ML researchers can\ncomprehend the field's current state more quickly and become productive.", "pdf_link": "http://arxiv.org/pdf/2307.10169v1"}, {"title": "ZeRO++: Extremely Efficient Collective Communication for Giant Model\n  Training", "abstract": "Zero Redundancy Optimizer (ZeRO) has been used to train a wide range of large\nlanguage models on massive GPUs clusters due to its ease of use, efficiency,\nand good scalability. However, when training on low-bandwidth clusters, or at\nscale which forces batch size per GPU to be small, ZeRO's effective throughput\nis limited because of high communication volume from gathering weights in\nforward pass, backward pass, and averaging gradients. This paper introduces\nthree communication volume reduction techniques, which we collectively refer to\nas ZeRO++, targeting each of the communication collectives in ZeRO. First is\nblock-quantization based all-gather. Second is data remapping that trades-off\ncommunication for more memory. Third is a novel all-to-all based quantized\ngradient averaging paradigm as replacement of reduce-scatter collective, which\npreserves accuracy despite communicating low precision data. Collectively,\nZeRO++ reduces communication volume of ZeRO by 4x, enabling up to 2.16x better\nthroughput at 384 GPU scale.", "pdf_link": "http://arxiv.org/pdf/2306.10209v1"}, {"title": "Efficient Memory Management for Large Language Model Serving with\n  PagedAttention", "abstract": "High throughput serving of large language models (LLMs) requires batching\nsufficiently many requests at a time. However, existing systems struggle\nbecause the key-value cache (KV cache) memory for each request is huge and\ngrows and shrinks dynamically. When managed inefficiently, this memory can be\nsignificantly wasted by fragmentation and redundant duplication, limiting the\nbatch size. To address this problem, we propose PagedAttention, an attention\nalgorithm inspired by the classical virtual memory and paging techniques in\noperating systems. On top of it, we build vLLM, an LLM serving system that\nachieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV\ncache within and across requests to further reduce memory usage. Our\nevaluations show that vLLM improves the throughput of popular LLMs by\n2-4$\\times$ with the same level of latency compared to the state-of-the-art\nsystems, such as FasterTransformer and Orca. The improvement is more pronounced\nwith longer sequences, larger models, and more complex decoding algorithms.\nvLLM's source code is publicly available at\nhttps://github.com/vllm-project/vllm", "pdf_link": "http://arxiv.org/pdf/2309.06180v1"}, {"title": "EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models", "abstract": "Large Language Models (LLMs) such as GPTs and LLaMa have ushered in a\nrevolution in machine intelligence, owing to their exceptional capabilities in\na wide range of machine learning tasks. However, the transition of LLMs from\ndata centers to edge devices presents a set of challenges and opportunities.\nWhile this shift can enhance privacy and availability, it is hampered by the\nenormous parameter sizes of these models, leading to impractical runtime costs.\nIn light of these considerations, we introduce EdgeMoE, the first on-device\ninference engine tailored for mixture-of-expert (MoE) LLMs, a popular variant\nof sparse LLMs that exhibit nearly constant computational complexity as their\nparameter size scales. EdgeMoE achieves both memory and computational\nefficiency by strategically partitioning the model across the storage\nhierarchy. Specifically, non-expert weights are stored in the device's memory,\nwhile expert weights are kept in external storage and are fetched into memory\nonly when they are activated. This design is underpinned by a crucial insight\nthat expert weights, though voluminous, are infrequently accessed due to sparse\nactivation patterns. To further mitigate the overhead associated with expert\nI/O swapping, EdgeMoE incorporates two innovative techniques: (1) Expert-wise\nbitwidth adaptation: This method reduces the size of expert weights with an\nacceptable level of accuracy loss. (2) Expert management: It predicts the\nexperts that will be activated in advance and preloads them into the\ncompute-I/O pipeline, thus further optimizing the process. In empirical\nevaluations conducted on well-established MoE LLMs and various edge devices,\nEdgeMoE demonstrates substantial memory savings and performance improvements\nwhen compared to competitive baseline solutions.", "pdf_link": "http://arxiv.org/pdf/2308.14352v1"}, {"title": "Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large\n  Language Models for Dynamic Inference Using Sorted Fine-Tuning (SoFT)", "abstract": "The rapid advancement of large language models (LLMs) has revolutionized\nnatural language processing (NLP). While these models excel at understanding\nand generating human-like text, their widespread deployment can be\nprohibitively expensive. SortedNet is a recent training technique for enabling\ndynamic inference for deep neural networks. It leverages network modularity to\ncreate sub-models with varying computational loads, sorting them based on\ncomputation/accuracy characteristics in a nested manner. We extend SortedNet to\ngenerative NLP tasks, making large language models dynamic without any\npretraining and by only replacing standard Supervised Fine-Tuning (SFT) with\nSorted Fine-Tuning (SoFT) at the same costs. Our approach boosts model\nefficiency, eliminating the need for multiple models for various scenarios\nduring inference. We show that using this approach, we are able to unlock the\npotential of intermediate layers of transformers in generating the target\noutput. Our sub-models remain integral components of the original model,\nminimizing storage requirements and transition costs between different\ncomputational/latency budgets. By applying this approach on LLaMa 2 13B for\ntuning on the Stanford Alpaca dataset and comparing it to normal tuning and\nearly exit via PandaLM benchmark, we show that Sorted Fine-Tuning can deliver\nmodels twice as fast as the original model while maintaining or exceeding\nperformance.", "pdf_link": "http://arxiv.org/pdf/2309.08968v1"}, {"title": "Blockwise Parallel Transformer for Large Context Models", "abstract": "Transformers have emerged as the cornerstone of state-of-the-art natural\nlanguage processing models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands posed by the\nself-attention mechanism and the large feedforward network in Transformers\nlimit their ability to handle long sequences, thereby creating challenges for\ntasks involving multiple long sequences or long-term dependencies. We present a\ndistinct approach, Blockwise Parallel Transformer (BPT), that leverages\nblockwise computation of self-attention and feedforward network fusion to\nminimize memory costs. By processing longer input sequences while maintaining\nmemory efficiency, BPT enables training sequences 32 times longer than vanilla\nTransformers and up to 4 times longer than previous memory-efficient methods.\nExtensive experiments on language modeling and reinforcement learning tasks\ndemonstrate the effectiveness of BPT in reducing memory requirements and\nimproving performance.", "pdf_link": "http://arxiv.org/pdf/2305.19370v3"}, {"title": "PaLM: Scaling Language Modeling with Pathways", "abstract": "Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.", "pdf_link": "http://arxiv.org/pdf/2204.02311v5"}, {"title": "SWARM Parallelism: Training Large Models Can Be Surprisingly\n  Communication-Efficient", "abstract": "Many deep learning applications benefit from using large models with billions\nof parameters. Training these models is notoriously expensive due to the need\nfor specialized HPC clusters. In this work, we consider alternative setups for\ntraining large models: using cheap \"preemptible\" instances or pooling existing\nresources from multiple regions. We analyze the performance of existing\nmodel-parallel algorithms in these conditions and find configurations where\ntraining larger models becomes less communication-intensive. Based on these\nfindings, we propose SWARM parallelism, a model-parallel training algorithm\ndesigned for poorly connected, heterogeneous and unreliable devices. SWARM\ncreates temporary randomized pipelines between nodes that are rebalanced in\ncase of failure. We empirically validate our findings and compare SWARM\nparallelism with existing large-scale training approaches. Finally, we combine\nour insights with compression strategies to train a large Transformer language\nmodel with 1B shared parameters (approximately 13B before sharing) on\npreemptible T4 GPUs with less than 200Mb/s network.", "pdf_link": "http://arxiv.org/pdf/2301.11913v2"}, {"title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of\n  Large Language Models", "abstract": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led\nto the development of numerous cost-effective and accessible alternatives that\nare created by finetuning open-access LLMs with task-specific data (e.g.,\nChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning\nmethods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly\none of the most attractive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs while achieving comparable or\neven better performance. To enable further research on PEFT methods of LLMs,\nthis paper presents LLM-Adapters, an easy-to-use framework that integrates\nvarious adapters into LLMs and can execute these adapter-based PEFT methods of\nLLMs for different tasks. The framework includes state-of-the-art open-access\nLLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as\nSeries adapters, Parallel adapter, Prompt-based learning and\nReparametrization-based methods. Moreover, we conduct extensive empirical\nstudies on the impact of adapter types, placement locations, and\nhyper-parameters to the best design for each adapter-based methods. We evaluate\nthe effectiveness of the adapters on fourteen datasets from two different\nreasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results\ndemonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few\nextra trainable parameters yields comparable, and in some cases superior,\nperformance to powerful LLMs (175B) in zero-shot inference on both reasoning\ntasks.", "pdf_link": "http://arxiv.org/pdf/2304.01933v3"}, {"title": "PoSE: Efficient Context Window Extension of LLMs via Positional\n  Skip-wise Training", "abstract": "Large Language Models (LLMs) are trained with a pre-defined context length,\nrestricting their use in scenarios requiring long inputs. Previous efforts for\nadapting LLMs to a longer length usually requires fine-tuning with this target\nlength (Full-length fine-tuning), suffering intensive training cost. To\ndecouple train length from target length for efficient context window\nextension, we propose Positional Skip-wisE (PoSE) training that smartly\nsimulates long inputs using a fixed context window. This is achieved by first\ndividing the original context window into several chunks, then designing\ndistinct skipping bias terms to manipulate the position indices of each chunk.\nThese bias terms and the lengths of each chunk are altered for every training\nexample, allowing the model to adapt to all positions within target length.\nExperimental results show that PoSE greatly reduces memory and time overhead\ncompared with Full-length fine-tuning, with minimal impact on performance.\nLeveraging this advantage, we have successfully extended the LLaMA model to\n128k tokens using a 2k training context window. Furthermore, we empirically\nconfirm that PoSE is compatible with all RoPE-based LLMs and position\ninterpolation strategies. Notably, our method can potentially support infinite\nlength, limited only by memory usage in inference. With ongoing progress for\nefficient inference, we believe PoSE can further scale the context window\nbeyond 128k.", "pdf_link": "http://arxiv.org/pdf/2309.10400v2"}, {"title": "Scaling Laws for Sparsely-Connected Foundation Models", "abstract": "We explore the impact of parameter sparsity on the scaling behavior of\nTransformers trained on massive datasets (i.e., \"foundation models\"), in both\nvision and language domains. In this setting, we identify the first scaling law\ndescribing the relationship between weight sparsity, number of non-zero\nparameters, and amount of training data, which we validate empirically across\nmodel and data scales; on ViT/JFT-4B and T5/C4. These results allow us to\ncharacterize the \"optimal sparsity\", the sparsity level which yields the best\nperformance for a given effective model size and training budget. For a fixed\nnumber of non-zero parameters, we identify that the optimal sparsity increases\nwith the amount of data used for training. We also extend our study to\ndifferent sparsity structures (such as the hardware-friendly n:m pattern) and\nstrategies (such as starting from a pretrained dense model). Our findings shed\nlight on the power and limitations of weight sparsity across various parameter\nand computational settings, offering both theoretical understanding and\npractical implications for leveraging sparsity towards computational efficiency\nimprovements.", "pdf_link": "http://arxiv.org/pdf/2309.08520v1"}, {"title": "S$^{3}$: Increasing GPU Utilization during Generative Inference for\n  Higher Throughput", "abstract": "Generating texts with a large language model (LLM) consumes massive amounts\nof memory. Apart from the already-large model parameters, the key/value (KV)\ncache that holds information about previous tokens in a sequence can grow to be\neven larger than the model itself. This problem is exacerbated in one of the\ncurrent LLM serving frameworks which reserves the maximum sequence length of\nmemory for the KV cache to guarantee generating a complete sequence as they do\nnot know the output sequence length. This restricts us to use a smaller batch\nsize leading to lower GPU utilization and above all, lower throughput. We argue\nthat designing a system with a priori knowledge of the output sequence can\nmitigate this problem. To this end, we propose S$^{3}$, which predicts the\noutput sequence length, schedules generation queries based on the prediction to\nincrease device resource utilization and throughput, and handle mispredictions.\nOur proposed method achieves 6.49$\\times$ throughput over those systems that\nassume the worst case for the output sequence length.", "pdf_link": "http://arxiv.org/pdf/2306.06000v1"}, {"title": "RWKV: Reinventing RNNs for the Transformer Era", "abstract": "Transformers have revolutionized almost all natural language processing (NLP)\ntasks but suffer from memory and computational complexity that scales\nquadratically with sequence length. In contrast, recurrent neural networks\n(RNNs) exhibit linear scaling in memory and computational requirements but\nstruggle to match the same performance as Transformers due to limitations in\nparallelization and scalability. We propose a novel model architecture,\nReceptance Weighted Key Value (RWKV), that combines the efficient\nparallelizable training of Transformers with the efficient inference of RNNs.\nOur approach leverages a linear attention mechanism and allows us to formulate\nthe model as either a Transformer or an RNN, which parallelizes computations\nduring training and maintains constant computational and memory complexity\nduring inference, leading to the first non-transformer architecture to be\nscaled to tens of billions of parameters. Our experiments reveal that RWKV\nperforms on par with similarly sized Transformers, suggesting that future work\ncan leverage this architecture to create more efficient models. This work\npresents a significant step towards reconciling the trade-offs between\ncomputational efficiency and model performance in sequence processing tasks.", "pdf_link": "http://arxiv.org/pdf/2305.13048v1"}, {"title": "Scaling Language Models: Methods, Analysis & Insights from Training\n  Gopher", "abstract": "Language modelling provides a step towards intelligent communication systems\nby harnessing large repositories of written human knowledge to better predict\nand understand the world. In this paper, we present an analysis of\nTransformer-based language model performance across a wide range of model\nscales -- from models with tens of millions of parameters up to a 280 billion\nparameter model called Gopher. These models are evaluated on 152 diverse tasks,\nachieving state-of-the-art performance across the majority. Gains from scale\nare largest in areas such as reading comprehension, fact-checking, and the\nidentification of toxic language, but logical and mathematical reasoning see\nless benefit. We provide a holistic analysis of the training dataset and\nmodel's behaviour, covering the intersection of model scale with bias and\ntoxicity. Finally we discuss the application of language models to AI safety\nand the mitigation of downstream harms.", "pdf_link": "http://arxiv.org/pdf/2112.11446v2"}, {"title": "LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models", "abstract": "We present LongLoRA, an efficient fine-tuning approach that extends the\ncontext sizes of pre-trained large language models (LLMs), with limited\ncomputation cost. Typically, training LLMs with long context sizes is\ncomputationally expensive, requiring extensive training hours and GPU\nresources. For example, training on the context length of 8192 needs 16x\ncomputational costs in self-attention layers as that of 2048. In this paper, we\nspeed up the context extension of LLMs in two aspects. On the one hand,\nalthough dense global attention is needed during inference, fine-tuning the\nmodel can be effectively and efficiently done by sparse local attention. The\nproposed shift short attention effectively enables context extension, leading\nto non-trivial computation saving with similar performance to fine-tuning with\nvanilla attention. Particularly, it can be implemented with only two lines of\ncode in training, while being optional in inference. On the other hand, we\nrevisit the parameter-efficient fine-tuning regime for context expansion.\nNotably, we find that LoRA for context extension works well under the premise\nof trainable embedding and normalization. LongLoRA demonstrates strong\nempirical results on various tasks on LLaMA2 models from 7B/13B to 70B.\nLongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a\nsingle 8x A100 machine. LongLoRA extends models' context while retaining their\noriginal architectures, and is compatible with most existing techniques, like\nFlashAttention-2. In addition, to make LongLoRA practical, we collect a\ndataset, LongQA, for supervised fine-tuning. It contains more than 3k long\ncontext question-answer pairs.", "pdf_link": "http://arxiv.org/pdf/2309.12307v1"}, {"title": "Gated recurrent neural networks discover attention", "abstract": "Recent architectural developments have enabled recurrent neural networks\n(RNNs) to reach and even surpass the performance of Transformers on certain\nsequence modeling tasks. These modern RNNs feature a prominent design pattern:\nlinear recurrent layers interconnected by feedforward paths with multiplicative\ngating. Here, we show how RNNs equipped with these two design elements can\nexactly implement (linear) self-attention, the main building block of\nTransformers. By reverse-engineering a set of trained RNNs, we find that\ngradient descent in practice discovers our construction. In particular, we\nexamine RNNs trained to solve simple in-context learning tasks on which\nTransformers are known to excel and find that gradient descent instills in our\nRNNs the same attention-based in-context learning algorithm used by\nTransformers. Our findings highlight the importance of multiplicative\ninteractions in neural networks and suggest that certain RNNs might be\nunexpectedly implementing attention under the hood.", "pdf_link": "http://arxiv.org/pdf/2309.01775v1"}, {"title": "From Sparse to Soft Mixtures of Experts", "abstract": "Sparse mixture of expert architectures (MoEs) scale model capacity without\nlarge increases in training or inference costs. Despite their success, MoEs\nsuffer from a number of issues: training instability, token dropping, inability\nto scale the number of experts, or ineffective finetuning. In this work, we\nproposeSoft MoE, a fully-differentiable sparse Transformer that addresses these\nchallenges, while maintaining the benefits of MoEs. Soft MoE performs an\nimplicit soft assignment by passing different weighted combinations of all\ninput tokens to each expert. As in other MoE works, experts in Soft MoE only\nprocess a subset of the (combined) tokens, enabling larger model capacity at\nlower inference cost. In the context of visual recognition, Soft MoE greatly\noutperforms standard Transformers (ViTs) and popular MoE variants (Tokens\nChoice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower\ninference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its\nperformance after similar training. Soft MoE also scales well: Soft MoE Huge/14\nwith 128 experts in 16 MoE layers has over 40x more parameters than ViT\nHuge/14, while inference time cost grows by only 2%, and it performs\nsubstantially better.", "pdf_link": "http://arxiv.org/pdf/2308.00951v1"}, {"title": "One Wide Feedforward is All You Need", "abstract": "The Transformer architecture has two main non-embedding components: Attention\nand the Feed Forward Network (FFN). Attention captures interdependencies\nbetween words regardless of their position, while the FFN non-linearly\ntransforms each input token independently. In this work we explore the role of\nthe FFN, and find that despite taking up a significant fraction of the model's\nparameters, it is highly redundant. Concretely, we are able to substantially\nreduce the number of parameters with only a modest drop in accuracy by removing\nthe FFN on the decoder layers and sharing a single FFN across the encoder.\nFinally we scale this architecture back to its original size by increasing the\nhidden dimension of the shared FFN, achieving substantial gains in both\naccuracy and latency with respect to the original Transformer Big.", "pdf_link": "http://arxiv.org/pdf/2309.01826v2"}, {"title": "LaMDA: Language Models for Dialog Applications", "abstract": "We present LaMDA: Language Models for Dialog Applications. LaMDA is a family\nof Transformer-based neural language models specialized for dialog, which have\nup to 137B parameters and are pre-trained on 1.56T words of public dialog data\nand web text. While model scaling alone can improve quality, it shows less\nimprovements on safety and factual grounding. We demonstrate that fine-tuning\nwith annotated data and enabling the model to consult external knowledge\nsources can lead to significant improvements towards the two key challenges of\nsafety and factual grounding. The first challenge, safety, involves ensuring\nthat the model's responses are consistent with a set of human values, such as\npreventing harmful suggestions and unfair bias. We quantify safety using a\nmetric based on an illustrative set of human values, and we find that filtering\ncandidate responses using a LaMDA classifier fine-tuned with a small amount of\ncrowdworker-annotated data offers a promising approach to improving model\nsafety. The second challenge, factual grounding, involves enabling the model to\nconsult external knowledge sources, such as an information retrieval system, a\nlanguage translator, and a calculator. We quantify factuality using a\ngroundedness metric, and we find that our approach enables the model to\ngenerate responses grounded in known sources, rather than responses that merely\nsound plausible. Finally, we explore the use of LaMDA in the domains of\neducation and content recommendations, and analyze their helpfulness and role\nconsistency.", "pdf_link": "http://arxiv.org/pdf/2201.08239v3"}, {"title": "Efficient Streaming Language Models with Attention Sinks", "abstract": "Deploying Large Language Models (LLMs) in streaming applications such as\nmulti-round dialogue, where long interactions are expected, is urgently needed\nbut poses two major challenges. Firstly, during the decoding stage, caching\nprevious tokens' Key and Value states (KV) consumes extensive memory. Secondly,\npopular LLMs cannot generalize to longer texts than the training sequence\nlength. Window attention, where only the most recent KVs are cached, is a\nnatural approach -- but we show that it fails when the text length surpasses\nthe cache size. We observe an interesting phenomenon, namely attention sink,\nthat keeping the KV of initial tokens will largely recover the performance of\nwindow attention. In this paper, we first demonstrate that the emergence of\nattention sink is due to the strong attention scores towards initial tokens as\na ``sink'' even if they are not semantically important. Based on the above\nanalysis, we introduce StreamingLLM, an efficient framework that enables LLMs\ntrained with a finite length attention window to generalize to infinite\nsequence lengths without any fine-tuning. We show that StreamingLLM can enable\nLlama-2, MPT, Falcon, and Pythia to perform stable and efficient language\nmodeling with up to 4 million tokens and more. In addition, we discover that\nadding a placeholder token as a dedicated attention sink during pre-training\ncan further improve streaming deployment. In streaming settings, StreamingLLM\noutperforms the sliding window recomputation baseline by up to 22.2x speedup.\nCode and datasets are provided at https://github.com/mit-han-lab/streaming-llm.", "pdf_link": "http://arxiv.org/pdf/2309.17453v1"}, {"title": "LightSeq: Sequence Level Parallelism for Distributed Training of Long\n  Context Transformers", "abstract": "Increasing the context length of large language models (LLMs) unlocks\nfundamentally new capabilities, but also significantly increases the memory\nfootprints of training. Previous model-parallel systems such as Megatron-LM\npartition and compute different attention heads in parallel, resulting in large\ncommunication volumes, so they cannot scale beyond the number of attention\nheads, thereby hindering its adoption. In this paper, we introduce a new\napproach, LightSeq, for long-context LLMs training. LightSeq has many notable\nadvantages. First, LightSeq partitions over the sequence dimension, hence is\nagnostic to model architectures and readily applicable for models with varying\nnumbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query\nattention. Second, LightSeq not only requires up to 4.7x less communication\nthan Megatron-LM on popular LLMs but also overlaps the communication with\ncomputation. To further reduce the training time, LightSeq features a novel\ngradient checkpointing scheme to bypass an forward computation for\nmemory-efficient attention. We evaluate LightSeq on Llama-7B and its variants\nwith sequence lengths from 32K to 512K. Through comprehensive experiments on\nsingle and cross-node training, we show that LightSeq achieves up to 1.24-2.01x\nend-to-end speedup, and a 2-8x longer sequence length on models with fewer\nheads, compared to Megatron-LM. Codes will be available at\nhttps://github.com/RulinShao/LightSeq.", "pdf_link": "http://arxiv.org/pdf/2310.03294v1"}, {"title": "GrowLength: Accelerating LLMs Pretraining by Progressively Growing\n  Training Length", "abstract": "The evolving sophistication and intricacies of Large Language Models (LLMs)\nyield unprecedented advancements, yet they simultaneously demand considerable\ncomputational resources and incur significant costs. To alleviate these\nchallenges, this paper introduces a novel, simple, and effective method named\n``\\growlength'' to accelerate the pretraining process of LLMs. Our method\nprogressively increases the training length throughout the pretraining phase,\nthereby mitigating computational costs and enhancing efficiency. For instance,\nit begins with a sequence length of 128 and progressively extends to 4096. This\napproach enables models to process a larger number of tokens within limited\ntime frames, potentially boosting their performance. In other words, the\nefficiency gain is derived from training with shorter sequences optimizing the\nutilization of resources. Our extensive experiments with various\nstate-of-the-art LLMs have revealed that models trained using our method not\nonly converge more swiftly but also exhibit superior performance metrics\ncompared to those trained with existing methods. Furthermore, our method for\nLLMs pretraining acceleration does not require any additional engineering\nefforts, making it a practical solution in the realm of LLMs.", "pdf_link": "http://arxiv.org/pdf/2310.00576v1"}, {"title": "An Efficient Sparse Inference Software Accelerator for Transformer-based\n  Language Models on CPUs", "abstract": "In recent years, Transformer-based language models have become the standard\napproach for natural language processing tasks. However, stringent throughput\nand latency requirements in industrial applications are limiting their\nadoption. To mitigate the gap, model compression techniques such as structured\npruning are being used to improve inference efficiency. However, most existing\nneural network inference runtimes lack adequate support for structured\nsparsity. In this paper, we propose an efficient sparse deep learning inference\nsoftware stack for Transformer-based language models where the weights are\npruned with constant block size. Our sparse software accelerator leverages\nIntel Deep Learning Boost to maximize the performance of sparse matrix - dense\nmatrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel\noutperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an\norder of magnitude on a wide range of GEMM shapes under 5 representative\nsparsity ratios (70%, 75%, 80%, 85%, 90%). Moreover, our SpMM kernel shows up\nto 5x speedup over dense GEMM kernel of oneDNN, a well-optimized dense library\nwidely used in industry. We apply our sparse accelerator on widely-used\nTransformer-based language models including Bert-Mini, DistilBERT, Bert-Base,\nand BERT-Large. Our sparse inference software shows up to 1.5x speedup over\nNeural Magic's Deepsparse under same configurations on Xeon on Amazon Web\nServices under proxy production latency constraints. We also compare our\nsolution with two framework-based inference solutions, ONNX Runtime and\nPyTorch, and demonstrate up to 37x speedup over ONNX Runtime and 345x over\nPyTorch on Xeon under the latency constraints. All the source code is publicly\navailable on Github: https://github.com/intel/intel-extension-for-transformers.", "pdf_link": "http://arxiv.org/pdf/2306.16601v1"}, {"title": "FlexGen: High-Throughput Generative Inference of Large Language Models\n  with a Single GPU", "abstract": "The high computational and memory requirements of large language model (LLM)\ninference make it feasible only with multiple high-end accelerators. Motivated\nby the emerging demand for latency-insensitive tasks with batched processing,\nthis paper initiates the study of high-throughput LLM inference using limited\nresources, such as a single commodity GPU. We present FlexGen, a\nhigh-throughput generation engine for running LLMs with limited GPU memory.\nFlexGen can be flexibly configured under various hardware resource constraints\nby aggregating memory and computation from the GPU, CPU, and disk. By solving a\nlinear programming problem, it searches for efficient patterns to store and\naccess tensors. FlexGen further compresses the weights and the attention cache\nto 4 bits with negligible accuracy loss. These techniques enable FlexGen to\nhave a larger space of batch size choices and thus significantly increase\nmaximum throughput. As a result, when running OPT-175B on a single 16GB GPU,\nFlexGen achieves significantly higher throughput compared to state-of-the-art\noffloading systems, reaching a generation throughput of 1 token/s for the first\ntime with an effective batch size of 144. On the HELM benchmark, FlexGen can\nbenchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21\nhours. The code is available at https://github.com/FMInference/FlexGen", "pdf_link": "http://arxiv.org/pdf/2303.06865v2"}, {"title": "Ring Attention with Blockwise Transformers for Near-Infinite Context", "abstract": "Transformers have emerged as the architecture of choice for many\nstate-of-the-art AI models, showcasing exceptional performance across a wide\nrange of AI applications. However, the memory demands imposed by Transformers\nlimit their ability to handle long sequences, thereby creating challenges for\ntasks involving extended sequences or long-term dependencies. We present a\ndistinct approach, Ring Attention, which leverages blockwise computation of\nself-attention to distribute long sequences across multiple devices while\noverlapping the communication of key-value blocks with the computation of\nblockwise attention. Ring Attention enables training and inference of sequences\nthat are up to device count times longer than those of prior memory-efficient\nTransformers, effectively eliminating the memory constraints imposed by\nindividual devices. Extensive experiments on language modeling tasks\ndemonstrate the effectiveness of Ring Attention in allowing large sequence\ninput size and improving performance.", "pdf_link": "http://arxiv.org/pdf/2310.01889v3"}, {"title": "OPT: Open Pre-trained Transformer Language Models", "abstract": "Large language models, which are often trained for hundreds of thousands of\ncompute days, have shown remarkable capabilities for zero- and few-shot\nlearning. Given their computational cost, these models are difficult to\nreplicate without significant capital. For the few that are available through\nAPIs, no access is granted to the full model weights, making them difficult to\nstudy. We present Open Pre-trained Transformers (OPT), a suite of decoder-only\npre-trained transformers ranging from 125M to 175B parameters, which we aim to\nfully and responsibly share with interested researchers. We show that OPT-175B\nis comparable to GPT-3, while requiring only 1/7th the carbon footprint to\ndevelop. We are also releasing our logbook detailing the infrastructure\nchallenges we faced, along with code for experimenting with all of the released\nmodels.", "pdf_link": "http://arxiv.org/pdf/2205.01068v4"}, {"title": "Retentive Network: A Successor to Transformer for Large Language Models", "abstract": "In this work, we propose Retentive Network (RetNet) as a foundation\narchitecture for large language models, simultaneously achieving training\nparallelism, low-cost inference, and good performance. We theoretically derive\nthe connection between recurrence and attention. Then we propose the retention\nmechanism for sequence modeling, which supports three computation paradigms,\ni.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel\nrepresentation allows for training parallelism. The recurrent representation\nenables low-cost $O(1)$ inference, which improves decoding throughput, latency,\nand GPU memory without sacrificing performance. The chunkwise recurrent\nrepresentation facilitates efficient long-sequence modeling with linear\ncomplexity, where each chunk is encoded parallelly while recurrently\nsummarizing the chunks. Experimental results on language modeling show that\nRetNet achieves favorable scaling results, parallel training, low-cost\ndeployment, and efficient inference. The intriguing properties make RetNet a\nstrong successor to Transformer for large language models. Code will be\navailable at https://aka.ms/retnet.", "pdf_link": "http://arxiv.org/pdf/2307.08621v4"}, {"title": "Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient\n  MoE for Instruction Tuning", "abstract": "The Mixture of Experts (MoE) is a widely known neural architecture where an\nensemble of specialized sub-models optimizes overall performance with a\nconstant computational cost. However, conventional MoEs pose challenges at\nscale due to the need to store all experts in memory. In this paper, we push\nMoE to the limit. We propose extremely parameter-efficient MoE by uniquely\ncombining MoE architecture with lightweight experts.Our MoE architecture\noutperforms standard parameter-efficient fine-tuning (PEFT) methods and is on\npar with full fine-tuning by only updating the lightweight experts -- less than\n1% of an 11B parameters model. Furthermore, our method generalizes to unseen\ntasks as it does not depend on any prior task knowledge. Our research\nunderscores the versatility of the mixture of experts architecture, showcasing\nits ability to deliver robust performance even when subjected to rigorous\nparameter constraints. Our code used in all the experiments is publicly\navailable here: https://github.com/for-ai/parameter-efficient-moe.", "pdf_link": "http://arxiv.org/pdf/2309.05444v1"}, {"title": "PanGu-\u03a3: Towards Trillion Parameter Language Model with Sparse\n  Heterogeneous Computing", "abstract": "The scaling of large language models has greatly improved natural language\nunderstanding, generation, and reasoning. In this work, we develop a system\nthat trained a trillion-parameter language model on a cluster of Ascend 910 AI\nprocessors and MindSpore framework, and present the language model with 1.085T\nparameters named PanGu-{\\Sigma}. With parameter inherent from PanGu-{\\alpha},\nwe extend the dense Transformer model to sparse one with Random Routed Experts\n(RRE), and efficiently train the model over 329B tokens by using Expert\nComputation and Storage Separation(ECSS). This resulted in a 6.3x increase in\ntraining throughput through heterogeneous computing. Our experimental findings\nshow that PanGu-{\\Sigma} provides state-of-the-art performance in zero-shot\nlearning of various Chinese NLP downstream tasks. Moreover, it demonstrates\nstrong abilities when fine-tuned in application data of open-domain dialogue,\nquestion answering, machine translation and code generation.", "pdf_link": "http://arxiv.org/pdf/2303.10845v1"}, {"title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme\n  Long Sequence Transformer Models", "abstract": "Computation in a typical Transformer-based large language model (LLM) can be\ncharacterized by batch size, hidden dimension, number of layers, and sequence\nlength. Until now, system works for accelerating LLM training have focused on\nthe first three dimensions: data parallelism for batch size, tensor parallelism\nfor hidden size and pipeline parallelism for model depth or layers. These\nwidely studied forms of parallelism are not targeted or optimized for long\nsequence Transformer models. Given practical application needs for long\nsequence LLM, renewed attentions are being drawn to sequence parallelism.\nHowever, existing works in sequence parallelism are constrained by\nmemory-communication inefficiency, limiting their scalability to long sequence\nlarge models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable\nand effective methodology for enabling highly efficient and scalable LLM\ntraining with extremely long sequence length. DeepSpeed-Ulysses at its core\npartitions input data along the sequence dimension and employs an efficient\nall-to-all collective communication for attention computation. Theoretical\ncommunication analysis shows that whereas other methods incur communication\noverhead as sequence length increases, DeepSpeed-Ulysses maintains constant\ncommunication volume when sequence length and compute devices are increased\nproportionally. Furthermore, experimental evaluations show that\nDeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the\nexisting method SOTA baseline.", "pdf_link": "http://arxiv.org/pdf/2309.14509v2"}, {"title": "Retrieval meets Long Context Large Language Models", "abstract": "Extending the context window of large language models (LLMs) is getting\npopular recently, while the solution of augmenting LLMs with retrieval has\nexisted for years. The natural questions are: i) Retrieval-augmentation versus\nlong context window, which one is better for downstream tasks? ii) Can both\nmethods be combined to get the best of both worlds? In this work, we answer\nthese questions by studying both solutions using two state-of-the-art\npretrained LLMs, i.e., a proprietary 43B GPT and LLaMA2-70B. Perhaps\nsurprisingly, we find that LLM with 4K context window using simple\nretrieval-augmentation at generation can achieve comparable performance to\nfinetuned LLM with 16K context window via positional interpolation on long\ncontext tasks, while taking much less computation. More importantly, we\ndemonstrate that retrieval can significantly improve the performance of LLMs\nregardless of their extended context window sizes. Our best model,\nretrieval-augmented LLaMA2-70B with 32K context window, outperforms\nGPT-3.5-turbo-16k and Davinci003 in terms of average score on seven long\ncontext tasks including question answering and query-based summarization. It\nalso outperforms its non-retrieval LLaMA2-70B-32k baseline by a margin, while\nbeing much faster at generation. Our study provides general insights on the\nchoice of retrieval-augmentation versus long context extension of LLM for\npractitioners.", "pdf_link": "http://arxiv.org/pdf/2310.03025v1"}, {"title": "Easy and Efficient Transformer : Scalable Inference Solution For large\n  NLP model", "abstract": "Recently, large-scale transformer-based models have been proven to be\neffective over various tasks across many domains. Nevertheless, applying them\nin industrial production requires tedious and heavy works to reduce inference\ncosts. To fill such a gap, we introduce a scalable inference solution: Easy and\nEfficient Transformer (EET), including a series of transformer inference\noptimization at the algorithm and implementation levels. First, we design\nhighly optimized kernels for long inputs and large hidden sizes. Second, we\npropose a flexible CUDA memory manager to reduce the memory footprint when\ndeploying a large model. Compared with the state-of-the-art transformer\ninference library (Faster Transformer v4.0), EET can achieve an average of\n1.40-4.20x speedup on the transformer decoder layer with an A100 GPU", "pdf_link": "http://arxiv.org/pdf/2104.12470v5"}, {"title": "Holistic Evaluation of Language Models", "abstract": "Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.", "pdf_link": "http://arxiv.org/pdf/2211.09110v2"}, {"title": "A Survey of Large Language Models", "abstract": "Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.", "pdf_link": "http://arxiv.org/pdf/2303.18223v12"}]